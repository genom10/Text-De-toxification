{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dmitriev Egor\n",
    "e.dmitriev@innopolis.university | BS20-RO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/genom10/.local/lib/python3.10/site-packages (4.35.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging>=20.0 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: filelock in /home/genom10/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/genom10/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec in /home/genom10/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/genom10/.local/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/genom10/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/genom10/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/genom10/.local/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/genom10/.local/lib/python3.10/site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/genom10/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "config = T5Config(vocab_size=250112, num_layers=8, num_heads=6)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/t5-small-ssm\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-small-ssm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/preprocessed.tsv', sep='\\t', index_col=0)\n",
    "df = df.iloc[:10000] # limit dataset\n",
    "df['reference'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(df['reference'].values.tolist(),\n",
    "                    text_target=df['translation'].values.tolist(),\n",
    "                    max_length=32,  # Specify the maximum sequence length\n",
    "                    padding=\"max_length\",  # Pad the input to the specified maximum length\n",
    "                    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "                    truncation=True  # Truncate the input if it exceeds the maximum length\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 168,   62,   54, 8179,   39,  280,    1,    0,    0,    0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
      "tensor([ 168,   62,  228, 8179,   39,  280,   21,   80,    1,    0])\n"
     ]
    }
   ],
   "source": [
    "print(encodings['input_ids'][0, :10])\n",
    "print(encodings['attention_mask'][0, :10])\n",
    "print(encodings['labels'][0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pt_encodings = torch.stack([encodings[key] for key in encodings.keys()]).swapaxes(0,1)\n",
    "pt_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 8000 9000\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "train_data.shape=torch.Size([8000, 3, 32])\n",
      "test_data.shape=torch.Size([1000, 3, 32])\n",
      "validation_data.shape=torch.Size([1000, 3, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 3, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "split = [0.8, 0.1, 0.1]\n",
    "total_samples = len(encodings.input_ids)\n",
    "train_split = int(split[0] * total_samples)\n",
    "test_split = int((split[0] + split[1]) * total_samples)\n",
    "print(total_samples, train_split, test_split)\n",
    "print(encodings.keys())\n",
    "\n",
    "\n",
    "train_data = pt_encodings[:train_split]\n",
    "test_data = pt_encodings[train_split:test_split]\n",
    "validation_data = pt_encodings[test_split:]\n",
    "\n",
    "print(f'{train_data.shape=}')\n",
    "print(f'{test_data.shape=}')\n",
    "print(f'{validation_data.shape=}')\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class T2TDataCollator:\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([unit[0] for unit in batch])\n",
    "        input_attention_mask = torch.stack([unit[2] for unit in batch])\n",
    "        output_ids = torch.stack([unit[1] for unit in batch])\n",
    "        output_ids[output_ids[:, :] == tokenizer.pad_token_id] = -100   \n",
    "        output_attention_mask = output_ids.apply_(lambda x : 0 if x == -100 else 1)\n",
    "        ret = torch.stack([input_ids, input_attention_mask, output_ids, output_attention_mask])\n",
    "        # print(ret.isnan().any())\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = T2TDataCollator()\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    validation_data, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "# next(iter(test_dataloader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def train(train_dataloader, n_epochs, learning_rate=0.001, print_every=100):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids = batch[0].to(device),\n",
    "                attention_mask = batch[1].to(device),\n",
    "                labels = batch[2].to(device),\n",
    "                # decoder_attention_mask = batch[3].to(device),\n",
    "        )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(loss)\n",
    "            print_loss_total += loss\n",
    "        print_loss_total /= len(train_dataloader)\n",
    "\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, (epoch+1) / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train(train_dataloader, 50, print_every=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
